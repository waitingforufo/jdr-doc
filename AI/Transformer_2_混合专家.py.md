# 混合专家（MoE）

引入动态选择的专家来扩展 Transformer 模型
两个主要结构：

- 专家
- 路由器
  ![](/img/2025-11-23_120052.png)
  ![](/img/2025-11-23_120309.png)

注意： 路由器是一个相对较小的模型，只有 32,000 个参数

Mixtro 确实有 8 个专家，但每个专家实际上有 56 亿个参数，而不是所说的 70 亿，很可能作者将共享参数加到了专家的参数中，这确实会导致 70 亿个参数

experts parameters
8 x 5.6b
因为共享参数不应该算在专家的参数中，因为那些不是专家特有的。
总的来说，专家们总共有 450 亿个参数，这是这个模型的大部分参数

![](/img/2025-11-23_121438.png)
![](/img/2025-11-23_122414.png)

# 现在已经对大型语言模型和变压器有了扎实的理解 不仅仅是黑盒，用这些只是更有效的使用它们
